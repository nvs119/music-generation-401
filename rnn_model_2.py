# -*- coding: utf-8 -*-
"""RNN Model 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kxBMhjlrD_zo4optjWyDATJhISPGDYlz

# Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/gdrive")
# %cd gdrive/Shareddrives/COS\ 401
# %cd dataset

import csv
import numpy
import os
import pandas as pd

import math
note_order = {'C0': 0, 'C#' : 1, 'Db': 1, 'C2': 2, 'D0': 2, 'D#': 3,
              'Eb': 3, 'E0': 4, 'E#': 5, 'Fb': 4, 'F0': 5, 'F#': 6,
              'Gb': 6, 'G0': 7, 'G#': 8, 'Ab': 8, 'A0': 9,
              'A#': 10, 'Bb': 10, 'B0': 11, 'B#': 0, 'Cb': 11, "rest": math.inf,
              'F2': 7, 'C2': 2, 'D2': 4, 'G2': 9, 'A2': 11, 'B-2': 9, 'B2': 1,
              'E2': 6, 'A-2': 7, 'C-2': 10, 'D-2': 0, 'E-2': 2, 'F-2': 3, "[]": -1}

def get_note_from_name(note_name):
  return note_order[note_name]


def get_standardized_note(note_name, key_fifths):
  if note_name == "rest":
    return -1
  note = get_note_from_name(note_name)
  return (note - 7*int(key_fifths) % 12) % 12

# standardize notes and chords in all train files

standardized_pds = []
for song in os.listdir("csv_train"):
  file_pd = pd.read_csv("csv_train/" + str(song))
  new_notes = []
  # print(song)

  for i in range(len(file_pd["note_root"])):
    new_notes.append(get_standardized_note(file_pd["note_root"][i], file_pd["key_fifths"][i]))

  file_pd["standardized_note"] = new_notes

  new_chords = []

  for i in range(len(file_pd["chord_root"])):
    new_chords.append(get_standardized_note(file_pd["chord_root"][i], file_pd["key_fifths"][i]))

  file_pd["standardized_chord"] = new_chords

  standardized_pds.append(file_pd)

# (note, chord, chord_type, octave)

chord_types_to_nums = {'major': 0, 'minor': 1, 'dominant':2,
          'major-seventh': 3, 'minor-seventh': 4,
          'major-sixth': 5, 'minor-sixth': 6, 
          'major-ninth': 7, 'minor-ninth': 8,
          'power': 9, 'half-diminished': 10, 
          'suspended-fourth': 11, 'dominant-13th': 12,
          'dominant-ninth': 13, 'nan': 14, '[]': 14, 'augmented':15,
          'augmented-seventh':16, 'maj': 0, 'min':1, 'maj7':3, '7':2,
          'diminished':17, 'dominant-seventh':2, 'diminished-seventh':18,
          'major-minor':19, 'maj69':20, 'dominant-11th':21, 'augmented-ninth':22,
          'min7':4, 'dim': 17, float("NaN"): 14, 'minor-major': 23}

nums_to_chord_types = {v:k for k, v in chord_types_to_nums.items()}

import string
note_dataset = []
import random

hello = []

for file_pd in standardized_pds:
  # note_dataset.append("13 major 13 13")
  notes_in_song = list(zip(file_pd["standardized_chord"], file_pd["chord_type"], file_pd["standardized_note"], file_pd["note_octave"]))
  # hello.append(set(file_pd["standardized_note"]))
  for note in notes_in_song:
    assert len(note) == 4
    new_note = []
    for i in note:
      if type(i) == str:
        new_note.append(i.strip())
      else:
        new_note.append(i)
    note = new_note
    final_note = []
    final_note.append(note[0])
    try:
      final_note.append(chord_types_to_nums[note[1]])
    except:
      if type(note[1]) == float:
        final_note.append(chord_types_to_nums['nan'])
      else:
        final_note.append(chord_types_to_nums['maj'])
    final_note.append(note[2])
    final_note.append(note[3])
    for i in final_note:
      if type(i) != int:
        print(i)
    if final_note[2] == -1:
      final_note[2] = 12
    note_dataset.append(final_note)

# print(hello[0].union(hello[1:]))

print(note_dataset[5])

# imports

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from sklearn.datasets import make_blobs
from sklearn.preprocessing import MinMaxScaler

import random

import tensorflow as tf

import numpy as np
import os
import time

"""# Test Blocks

## test block
"""

NUM_CLASSES = 5
NUM_FEATURES = 3

NUM_DATAPOINTS = 10


X = []
num_datapoints = 10

seq_len = 5

for i in range(NUM_CLASSES - 1):
  for datapoint in range(NUM_DATAPOINTS):
    features = []
    for j in range(NUM_FEATURES):
      if j == 0:
        features.append(i + round(random.random()))
      elif j == 1:
        features.append(NUM_CLASSES - 2 - i + round(random.random()))
      else:
        features.append(i + round(random.random()))
    X.append(features)

X = tf.convert_to_tensor(X)

print(X)

Xa = tf.data.Dataset.from_tensor_slices(X)
Xa = Xa.batch(seq_len + 1, drop_remainder=True)

for i in Xa.take(1):
  print(i)

print(len(list(Xa)))

def split_input_target(sequence):
    input_text = sequence[:-1]
    #target_text = sequence[1:]
    target_text = sequence[-1][0]
    return input_text, target_text


#dataset = Xa.map(lambda x: (x[:-1], x[1:]))

dataset = Xa.map(split_input_target)

BATCH_SIZE = 2
BUFFER_SIZE = 10000

dataset = (
    dataset
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True)
    .prefetch(tf.data.experimental.AUTOTUNE))
dataset


for i in dataset.take(1):
  print(i)

model = Sequential()
model.add(keras.layers.GRU(32, input_shape = (NUM_CLASSES, NUM_FEATURES)))
model.add(Dense(100, activation='relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(NUM_CLASSES, activation='softmax'))

model.compile(loss="sparse_categorical_crossentropy", optimizer='adam')

model.fit(dataset, epochs=100, verbose=0)

model.predict([[[1, 4, 1],
        [1, 4, 1],
        [1, 4, 0],
        [0, 4, 1],
        [0, 4, 1]]])

"""## test block 2"""

# NUM_CLASSES = 5
# NUM_FEATURES = 3

# NUM_DATAPOINTS = 10


# X = []
# num_datapoints = 10

# seq_len = 5

# for i in range(NUM_CLASSES - 1):
#   for datapoint in range(NUM_DATAPOINTS):
#     features = []
#     for j in range(NUM_FEATURES):
#       if j == 0:
#         features.append(i + round(random.random()))
#       elif j == 1:
#         features.append(NUM_CLASSES - 2 - i + round(random.random()))
#       else:
#         features.append(i + round(random.random()))
#     X.append(features)

# X = tf.convert_to_tensor(X)

# print(X)

# Xa = tf.data.Dataset.from_tensor_slices(X)
# Xa = Xa.batch(seq_len + 1, drop_remainder=True)

# for i in Xa.take(1):
#   print(i)

# print(len(list(Xa)))

# def split_input_target(sequence):
#     input_text = sequence[:-1]
#     #target_text = sequence[1:]
#     target_text = sequence[-1]
#     return input_text, target_text


# #dataset = Xa.map(lambda x: (x[:-1], x[1:]))

# dataset = Xa.map(split_input_target)

# BATCH_SIZE = 2
# BUFFER_SIZE = 10000

# dataset = (
#     dataset
#     .shuffle(BUFFER_SIZE)
#     .batch(BATCH_SIZE, drop_remainder=True)
#     .prefetch(tf.data.experimental.AUTOTUNE))
# dataset


# for i in dataset.take(1):
#   print(i)

# model = Sequential()
# model.add(keras.layers.GRU(32, input_shape = (NUM_CLASSES, NUM_FEATURES)))
# model.add(Dense(100, activation='relu'))
# model.add(Dense(100, activation='relu'))
# model.add(Dense(NUM_CLASSES, activation='softmax'))

# model.compile(loss="sparse_categorical_crossentropy", optimizer='adam')

# model.fit(dataset, epochs=100, verbose=0)

# model.predict([[[1, 4, 1],
#         [1, 4, 1],
#         [1, 4, 0],
#         [0, 4, 1],
#         [0, 4, 1]]])

"""# Building Dataset"""

seq_length = 80
note_dataset_tensor = tf.convert_to_tensor(note_dataset)

note_dataset_temp = tf.data.Dataset.from_tensor_slices(note_dataset_tensor)

sequences = note_dataset_temp.batch(seq_length+1, drop_remainder=True)

for seq in sequences.take(1):
  print(seq)

list_seq = list(sequences)

list_seq[232]

print(len(list(sequences)))

def split_chord(sequence):
    input_text = sequence[:-1]
    #target_text = sequence[1:]
    target_text = sequence[-1][0]
    return input_text, target_text
def split_chord_type(sequence):
    input_text = sequence[:-1]
    target_text = sequence[-1][1]
    return input_text, target_text
def split_note(sequence):
    input_text = sequence[:-1]
    target_text = sequence[-1][2]
    return input_text, target_text
def split_octave(sequence):
    input_text = sequence[:-1]
    target_text = sequence[-1][3]
    return input_text, target_text

#dataset = Xa.map(lambda x: (x[:-1], x[1:]))

dataset_chord = sequences.map(split_chord)
dataset_chord_type = sequences.map(split_chord_type)
dataset_note = sequences.map(split_note)
dataset_octave = sequences.map(split_octave)


datasets = [dataset_chord, dataset_chord_type, dataset_note, dataset_octave]

dataset = dataset_note


BATCH_SIZE = 32
BUFFER_SIZE = 10000


print(dataset)

for seq in dataset.take(1):
  print(seq)

dataset = (
    dataset
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True)
    .prefetch(tf.data.experimental.AUTOTUNE))


for i in range(len(datasets)):
  datasets[i] = (
    datasets[i]
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True)
    .prefetch(tf.data.experimental.AUTOTUNE))

"""# Create and Train Model"""

NUM_FEATURES = 4

def create_model(num_classes):
  model = Sequential()
  model.add(keras.layers.GRU(1024, input_shape = (seq_length, NUM_FEATURES)))
  model.add(Dense(100, activation='relu'))
  model.add(Dense(num_classes, activation='softmax'))
  model.compile(loss="sparse_categorical_crossentropy", optimizer='adam')
  return model

class_nums = [13, 30, 13, 10]
models = []

for i in range(len(datasets)):
  model = create_model(class_nums[i])
  print("next")
  model.fit(datasets[i], epochs=50, verbose=1)
  models.append(model)

models[1].summary()

tf.reshape(list_seq[232][:-1], [1, seq_length, 4])

model.predict(tf.reshape(list_seq[232][:-1], [1, seq_length, 4]))

null_init = [[[-1 for i in range(4)] for j in range(seq_length)]]

np.argmax(models[3].predict(null_init))

"""# Making Predictions

## Test Prediction
"""

# initial_notes = [[[ 0,  0,  4,  4],
#        [ 2,  0,  7,  4],
#        [ 2,  0, 6,  3],
#        [ 0,  2,  7,  4],
#        [ 3,  2,  5,  4],
#        [ 2,  0,  5,  4],
#        [ 5,  0,  9,  4],
#        [ 4,  0,  9,  4],
#        [ 0,  0, 7,  4],
#        [ 0,  0,  4,  4]]]

initial_notes = tf.reshape(list_seq[232][:-1], [1, seq_length, 4])

next_note = []
for j in range(len(models)):
  prediction = models[j].predict(initial_notes)
  prediction_std = []
  print(prediction[0])
  for k in list(prediction[0]):
    k = np.log(k) + 10
    if k < 0:
      k = 0
    # print(k)
    prediction_std.append(k)
  prediction = np.array(prediction_std)
  prediction = prediction / np.sum(prediction)
  real = int(np.random.choice(a = range(len(prediction_std)), size = 1, p = prediction))
  next_note.append(real)

next_note

"""## Notes and Chords"""

NOTES_IN_CHORD = {'major': [0, 2, 4, 5, 7, 9], 'minor': [0, 2, 3, 5, 7, 8, 10], 'dominant':[0, 2, 4, 5, 7, 9, 10],
              'major-seventh': [0, 2, 4, 5, 7, 9, 11], 'minor-seventh': [0, 2, 3, 5, 7, 8, 10],
          'major-sixth':[0, 2, 4, 5, 7, 9], 'minor-sixth':[0, 2, 3, 5, 7, 8, 10], 
          'major-ninth':[0, 2, 4, 5, 7, 9], 'minor-ninth': [0, 2, 3, 5, 7, 8, 10],
          'power': [0, 2, 4, 5, 7, 9], 'half-diminished': [0, 2, 3, 5, 6, 8, 10], 
          'diminished': [0, 2, 3, 5, 6, 8, 9], 
          'suspended-fourth': [0, 2, 5, 7, 9], 'dominant-13th': [0, 2, 4, 5, 7, 9, 10],
          'dominant-ninth': [0, 2, 4, 5, 7, 9, 10], 'nan': [0, 2, 4, 5, 7, 9], '[]': [0, 2, 4, 5, 7, 9], 
          'diminished-seventh': [0, 2, 3, 5, 6, 8, 9]}


chord_types_to_nums = {'major': 0, 'minor': 1, 'dominant':2,
          'major-seventh': 3, 'minor-seventh': 4,
          'major-sixth': 5, 'minor-sixth': 6, 
          'major-ninth': 7, 'minor-ninth': 8,
          'power': 9, 'half-diminished': 10, 
          'suspended-fourth': 11, 'dominant-13th': 12,
          'dominant-ninth': 13, 'nan': 14, '[]': 14, 'augmented':15,
          'augmented-seventh':16, 'maj': 0, 'min':1, 'maj7':3, '7':2,
          'diminished':17, 'dominant-seventh':2, 'diminished-seventh':18,
          'major-minor':19, 'maj69':20, 'dominant-11th':21, 'augmented-ninth':22,
          'min7':4, 'dim': 17, float("NaN"): 14, 'minor-major': 23}

# nums_to_chord_types = {v:k for k, v in chord_types_to_nums.items()}

nums_to_chord_types = {}
for i in chord_types_to_nums.keys():
  if i in NOTES_IN_CHORD.keys():
    nums_to_chord_types[chord_types_to_nums[i]] = i
  else:
    if chord_types_to_nums[i] not in nums_to_chord_types.keys():
      nums_to_chord_types[chord_types_to_nums[i]] = i

nums_to_chord_types[0]

a = [[list(j.numpy()) for j in i] for i in list(tf.reshape(list_seq[232][:-1], [1, seq_length, 4]))]
a

a = [[[int(k) for k in list(j.numpy())] for j in i] for i in list(tf.reshape(list_seq[232][:-1], [1, seq_length, 4]))]
a

"""## Post-Processing"""

song_len = 25

# initial_notes = [[[ 0,  0,  4,  4],
#        [ 0,  0,  7,  4],
#        [ 0,  0, 11,  3],
#        [ 0,  2,  0,  4],
#        [ 0,  2,  0,  4],
#        [ 5,  0,  5,  4],
#        [ 5,  0,  9,  4],
#        [ 5,  0,  5,  4],
#        [ 0,  0,  4,  4],
#        [ 0,  0,  7,  4]]]


initial_notes = [[[int(k) for k in list(j.numpy())] for j in i] for i in list(tf.reshape(list_seq[random.randint(1, 500)][:-1], [1, seq_length, 4]))]

favored_chords = [1, 2, 3, 4, 11]

dissonant_notes = [1, 6, 8, 10]

MOSH_FACTOR = 100

new_song = []
# current_notes = [[[-1 for i in range(4)] for j in range(10)]]
current_notes = initial_notes

same_note_penalty = 100

# 1 for default, 2 for random, 3 for log random, 4 for post-processing
METHOD_NUM = 4


TEMPERATURE = 1

for i in range(song_len):
  next_note = []
  for j in range(len(models)):
    if METHOD_NUM == 1:
      # old method
      next_note.append(np.argmax(models[j].predict(np.array(current_notes))))
    elif METHOD_NUM == 2:
      # new method 2
      prediction = models[j].predict(initial_notes)[0]
      real = int(np.random.choice(a = range(len(prediction)), size = 1, p = prediction))
      next_note.append(real)
      # new method 2 end
    elif METHOD_NUM == 3:
    # new method begin
      prediction = models[j].predict(initial_notes)
      prediction_std = []
      for k in list(prediction[0]):
        if k != 0:
          k = np.log(k) + TEMPERATURE
        if k < 0:
          k = 0
        # if k < 0.01:
        #   k = 0
        # k = k ** 0.5
        prediction_std.append(k)
      prediction = np.array(prediction_std)
      prediction = prediction / np.sum(prediction)
      real = int(np.random.choice(a = range(len(prediction_std)), size = 1, p = prediction))
      # octave correction
      if j == 3 and real in [0, 1, 2]:
        real = 4
      if j == 3 and real in [6, 7, 8]:
        real = 5
      if j == 3 and real == 5 and next_note[2] > 6:
        real = 4
      next_note.append(real)
    # new method end  
    elif METHOD_NUM == 4:
      # new method 3 begin
      prediction = models[j].predict(initial_notes)
      prediction_std = []
      prediction_std_old = []
      for k in list(prediction[0]):
        if k != 0:
          # this can be seen as the randomness factor
          k0 = np.log(k) + TEMPERATURE
          k1 = np.log(k) + 20
          k = k0
        if k1 < 0:
          k1 = 0
        if k < 0:
          k = k1 / (500 / TEMPERATURE)
        prediction_std.append(k)
        prediction_std_old.append(k1)
      
      prediction_old = np.array(prediction_std_old)
      # print(prediction_std)
      prediction = np.array(prediction_std)
      # prediction = np.power(prediction, 2)

      if j == 1:
        prediction[favored_chords] *= min(1, (MOSH_FACTOR /10))

      # if j == 2:
      #   prediction[dissonant_notes] /= MOSH_FACTOR

      # note in chord
      if j == 2:
        # print(np.floor(prediction))
        notes_in_chord = (np.array(NOTES_IN_CHORD[nums_to_chord_types[next_note[1]]]) + next_note[0]) % 12
        # print(notes_in_chord)
        # if np.all(prediction[notes_in_chord] == 0):
        #   prediction = prediction_old
        #   prediction[notes_in_chord] += MOSH_FACTOR
        #   prediction = np.power(prediction, 2)
        #   print("hello")
        prediction[notes_in_chord] = prediction[notes_in_chord] * MOSH_FACTOR
        # print(np.floor(prediction))


      # same note penalty
      if j in [0, 2] and i > 0:
        current_nota = 0
        previous_nota = 0
        penalty_here = 1
        for k in range(len(new_song) - 1, 0, -1):
          current_nota = new_song[k][j]
          if current_nota == previous_nota:
            penalty_here *= same_note_penalty
            previous_nota = current_nota
          elif previous_nota != 0:
            break
          else:
            previous_nota = current_nota
            pass
        # print(penalty_here)
        # print(prediction)
        prediction[new_song[i-1][j]] /= penalty_here
        # print(prediction)


      prediction = prediction / np.sum(prediction)


      real = int(np.random.choice(a = range(len(prediction_std)), size = 1, p = prediction))

      # octave correction
      if j == 3 and real in [0, 1, 2]:
        real = 4
      if j == 3 and real in [6, 7, 8]:
        real = 5
      if j == 3 and real == 5 and next_note[2] > 6:
        real = 4

      if j == 3 and i > 0:
        if next_note[2] - new_song[i - 1][2] + 12*(real - new_song[i - 1][3]) > 8:
          if abs(next_note[2] - new_song[i - 1][2]) < 8:
            real = new_song[i - 1][3]
          elif next_note[2] > new_song[i - 1][2]:
            real = new_song[i - 1][3] - 1
          else:
            real = new_song[i - 1][3] + 1

      next_note.append(real)
    # new method 3 end  
  new_song.append(next_note)
  current_notes[0].pop(0)
  current_notes[0].append(next_note)
  # print(current_notes)
  # print(current_notes)

new_song

"""# Play Music

We used Google Magenta for music playback (not for learning). The installation and import statements were taken from:

https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb
"""

import pandas as pd

chords = []
chord_types = []
notes = []
octaves = []
measures = []
duration = []
count = 0
measure = 0
for note_tf in new_song:
  # note_str = note_tf[0].numpy().decode('utf-8')
  # splitup = note_str.split(" ")
  # if splitup[0] in ["", " "]:
  #   splitup.pop(0)
  chords.append(int(note_tf[0]))

  chord_types.append(nums_to_chord_types.get(note_tf[1]))
  notes.append(int(note_tf[2]))
  octaves.append(int(note_tf[3]))
  duration.append(4)
  if count % 4 == 0:
    measure += 1
  count += 1
  measures.append(measure)



predict_pd = pd.DataFrame(list(zip(chords, chord_types, notes, octaves, duration, measures)),
                      columns = ["standardized_chord", "chord_type", "standardized_note", "note_octave", 
                      "note_duration", "measure"])

predict_pd

#at test {"output": "ignore"}
print('Installing dependencies...')

!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev
!pip install -qU pyfluidsynth pretty_midi

!pip install -qU magenta

# Hack to allow python to pick up the newly-installed fluidsynth lib. 
# This is only needed for the hosted Colab environment.
import ctypes.util
orig_ctypes_util_find_library = ctypes.util.find_library
def proxy_find_library(lib):
  if lib == 'fluidsynth':
    return 'libfluidsynth.so.1'
  else:
    return orig_ctypes_util_find_library(lib)
ctypes.util.find_library = proxy_find_library

print('Importing libraries and defining some helper functions...')
from google.colab import files

import magenta
import note_seq
import tensorflow

print('ðŸŽ‰ Done!')
print(magenta.__version__)
print(tensorflow.__version__)

from google.colab import files

import magenta
import note_seq
import tensorflow

from note_seq.protobuf import music_pb2

CHORDS_KEY = {'major': [4, 7], 'maj': [4, 7], 'min': [3, 7], 'minor':[3, 7], 'dominant':[4, 7, 10],
              'maj7': [4, 7, 11], 'min7': [3, 7, 10], 'dominant-seventh': [4, 7, 10],
          'major-seventh': [4, 7, 11], 'minor-seventh': [3, 7, 10],
          'major-sixth':[4, 7, 9], 'minor-sixth':[3, 7, 8], 
          'major-ninth':[4, 7, 10, 14], 'minor-ninth': [3, 7, 8, 14],
          'power': [5], 'half-diminished': [3, 7, 10], 
          'suspended-fourth': [5, 7], 'dominant-13th': [3, 5, 7, 9],
          'dominant-ninth': [4, 7, 9, 14], 'nan': [], '[]': [], 'dim': [3, 6, 9]}

DURATION_SCALE = 0.1
def play_song(file_pd):
  song_song = music_pb2.NoteSequence()
  current_measure = 1
  current_chord_duration = 0
  current_time = 0
  for i in range(len(file_pd["standardized_note"])):
    if file_pd["standardized_note"][i] == 13:
      continue
    # add chord
    if file_pd["measure"][i] > current_measure:
      current_measure += 1
      chord_duration = current_chord_duration
      current_chord_duration = 0
      chord_start_pitch = file_pd["standardized_chord"][i-1] + 12*4
      chord_pitches = [chord_start_pitch]
      if file_pd["chord_type"][i-1] in CHORDS_KEY.keys():
        for j in CHORDS_KEY[file_pd["chord_type"][i-1]]:
          chord_pitches.append(chord_start_pitch + j)
      else:
        print("chord not registered yet")
        print(file_pd["chord_type"][i-1])
      for pitch1 in chord_pitches:
        song_song.notes.add(pitch=pitch1, start_time= current_time - chord_duration, end_time = current_time - 0.001, velocity=60)
        pass
      
    # add note
    pitch = file_pd["standardized_note"][i] + 12*(file_pd["note_octave"][i] + 1)
    note_duration = file_pd["note_duration"][i] * DURATION_SCALE
    current_time += note_duration
    current_chord_duration += note_duration
    if file_pd["standardized_note"][i] != -1:
      song_song.notes.add(pitch=pitch, start_time= current_time - note_duration, end_time = current_time, velocity=60, instrument = 2, program = 1)
      pass
  
  song_song.total_time = current_time
  song_song.tempos.add(qpm=60);

  note_seq.plot_sequence(song_song)
  # This is a colab utility method that plays a NoteSequence.
  note_seq.play_sequence(song_song,synth=note_seq.fluidsynth)

play_song(predict_pd)